# Import necessary libraries
import pandas as pd
import numpy as np
import re

from torchmetrics.text.rouge import ROUGEScore
from evaluate import load

class Copying():
    '''
    This class constructs a model that is common in data-to-text generation research to outline the flaws of n-gram metrics.

    Simply put, this model removes any source data-specific formatting and then passes the resulting source data as the generated target text.
    '''
    def __init__(self, ds, file = "temp"):
        self.name = file
        self.n = 0
        self.ds = ds
        self.cl = False

    def build(self, train, dev):
        '''
        Typically the build function is where the model is trained, but this model is a copy-and-paste approach.

        Instead, we display the results of the model on the training and development datasets.
        '''
        print("Evaluating the copy method on the training split")
        self.gen(train, "train", split = "train")
        
        print("Evaluating the copy method on the dev split")
        self.gen(dev, "dev", split = "dev")

    def gen(self, data, seen_category, split = "test"):
        '''
        The gen function sets-up the process for preparing text generated by the model.

        This process differs between datasets,so there's a helper function for each one.
        '''
        if (seen_category == True):
            seen_category = data["seen_category"].unique()[0]

        def prep_webnlg(src):
            # Helper function to remove the WebNLG specific formatting
            src = src.replace(" | ", " ")
            src = src.replace(" [SEP] ", " ")
            src = src.replace('""', "")
            return src
        
        def prep_e2e(src):
            # Helper function to remove the E2E specific formatting
            src = re.findall(r'\[.*?\]', src)
            src = " ".join(src)
            src = src.replace("[", "")
            src = src.replace("]", "")
            return src

        # Initialize lists for the references and then the predicted text
        references, predictions = [], []

        if (self.ds == "webnlg"):
            # Add padding as the library needs the same number of references for every test case.
            padded_refs = 5
            predictions.extend([prep_webnlg(i) for i in data.input])
            references.extend([o.split(" <N> ") for o in data.output])
        else: # self.ds == "e2e"
            padded_refs = 45
            predictions.extend([prep_e2e(i) for i in data.input])
            references.extend([o.split(" <N> ") for o in data.output])

        if (split == "test"):
            for r, ref in enumerate(references):
                if (len(ref) < padded_refs):
                    references[r] = ref + [""] * (padded_refs - len(ref))

        if (split == "test"):
            return {"predictions": predictions, "references": references, "observed": seen_category}
        else:
            self.ngram_eval({"predictions": predictions, "references": references, "observed": seen_category}, split = split)

    def ngram_eval(self, pred_ref, split = "test"):
        '''
        Using n-gram and learning-based evaluation metrics, record and report the scores of each model's performance

        Save to a .csv file for tracking
        '''
        sacrebleu = load("sacrebleu")
        rouge = ROUGEScore()
        meteor = load("meteor")

        predictions = pred_ref["predictions"]
        references = pred_ref["references"]

        bleu = sacrebleu.compute(predictions = predictions, references = references,
                                 tokenize = "13a")
        print("\nThe corpus bleu score is:", bleu["score"],"\n")

        rouge_scores = rouge(predictions, references)
        print("These are the ROUGE score metrics:")
        print("rougeL F measure =", rouge_scores["rougeLsum_fmeasure"].item())
        print("rougeL precision =", rouge_scores["rougeLsum_precision"].item())
        print("rougeL recall =", rouge_scores["rougeLsum_recall"].item(),"\n")

        meteor_score = meteor.compute(predictions = predictions, references = references)
        print("The meteor score is:", meteor_score["meteor"],"\n")

        if (split == "test"):
            # ADD YOUR PATH TO RESULTS HERE
            results = pd.read_csv("" + self.ds + "_results.csv", header = 0, index_col = None)
            to_add = {}
            to_add["model"] = [self.name]
            to_add["bleu"] = [np.round(bleu["score"], 4)]
            to_add["rougeL_F"] = [np.round(rouge_scores["rougeLsum_fmeasure"].item() * 100, 4)]
            to_add["rougeL_P"] = [np.round(rouge_scores["rougeLsum_precision"].item() * 100, 4)]
            to_add["rougeL_R"] = [np.round(rouge_scores["rougeLsum_recall"].item() * 100, 4)]
            to_add["meteor"] = [np.round(meteor_score["meteor"] * 100, 4)]
            to_add["seen"] = [pred_ref["observed"]]
            to_add["n"] = [self.n]
            to_add["ds"] = [self.ds]
            to_add["cl"] = [self.cl]
            results = pd.concat([results, pd.DataFrame.from_dict(to_add)])
            # ADD YOUR PATH TO RESULTS HERE
            results.to_csv("" + self.ds + "_results.csv", header = True, index = False)